{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsZE19j08EdD"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOiUs_-b8EdH"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHDLn6on8EdI"
      },
      "source": [
        "\n",
        "Introducing FP8 precision training for faster RL inference. [Read Blog](https://docs.unsloth.ai/new/fp8-reinforcement-learning).\n",
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIvQnCDB8EdI"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nj6fDp5k8EdI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I0GIyDEnG7GS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# These are mamba kernels and we must have these for faster training\n",
        "#!pip install --no-build-isolation mamba_ssm==2.2.5\n",
        "#!pip install --no-build-isolation causal_conv1d==1.5.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMWlrRdzwgf"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xbb0cuLzwgf",
        "outputId": "f6d09941-e975-4296-9198-e40d250cf37a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.12.1: Fast Granitemoehybrid patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/granite-4.0-micro\",\n",
        "    \"unsloth/granite-4.0-h-micro\",\n",
        "    \"unsloth/granite-4.0-h-tiny\",\n",
        "    \"unsloth/granite-4.0-h-small\",\n",
        "\n",
        "    # Base pretrained Granite 4 models\n",
        "    \"unsloth/granite-4.0-micro-base\",\n",
        "    \"unsloth/granite-4.0-h-micro-base\",\n",
        "    \"unsloth/granite-4.0-h-tiny-base\",\n",
        "    \"unsloth/granite-4.0-h-small-base\",\n",
        "\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Phi-4\",\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/granite-4.0-350m\",\n",
        "    max_seq_length = 1024,   # Increased max_seq_length to 1024\n",
        "    load_in_4bit = True,    # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False,    # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update a small amount of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "a853d73f-5b93-4ef4-be3b-d7dbf0ac7031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel # Added import here for robustness\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Increased LoRA rank to 16\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "                      \"shared_mlp.input_linear\", \"shared_mlp.output_linear\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkq4RvEq7FQr",
        "outputId": "98a38993-906c-4949-c02a-d8c222f75d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "–ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:\n",
            "{'instruction': '–ß—Ç–æ —Ç–∞–∫–æ–µ –∫–∞–ø–µ–ª—å–Ω–∞—è —Ç—Ä—É–±–∫–∞ –∏ —á–µ–º –æ–Ω–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –∫–∞–ø–µ–ª—å–Ω–æ–π –ª–µ–Ω—Ç—ã?', 'input': '', 'output': '–ö–∞–ø–µ–ª—å–Ω–∞—è —Ç—Ä—É–±–∫–∞ (drip line) - —ç—Ç–æ –≥–∏–±–∫–∞—è —Ç—Ä—É–±–∫–∞ –∫—Ä—É–≥–ª–æ–≥–æ —Å–µ—á–µ–Ω–∏—è —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∏–ª–∏ –≤–Ω–µ—à–Ω–∏–º–∏ –∫–∞–ø–µ–ª—å–Ω–∏—Ü–∞–º–∏-—ç–º–∏—Ç—Ç–µ—Ä–∞–º–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–º–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—É—é –ø–æ–¥–∞—á—É –≤–æ–¥—ã –∫ –∫–æ—Ä–Ω—è–º —Ä–∞—Å—Ç–µ–Ω–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –∫–∞–ø–µ–ª—å–Ω–æ–π –ª–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ç–æ–Ω–∫–æ—Å—Ç–µ–Ω–Ω—É—é –ø–ª–æ—Å–∫—É—é —Ç—Ä—É–±–∫—É, —Ä–∞—Å–ø—Ä–∞–≤–ª—è—é—â—É—é—Å—è –ø–æ–¥ –¥–∞–≤–ª–µ–Ω–∏–µ–º, –∫–∞–ø–µ–ª—å–Ω–∞—è —Ç—Ä—É–±–∫–∞ –∏–º–µ–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –∫—Ä—É–≥–ª–æ–µ —Å–µ—á–µ–Ω–∏–µ –∏ –±–æ–ª–µ–µ –ø—Ä–æ—á–Ω—É—é –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é. –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è: –∫–∞–ø–µ–ª—å–Ω–∞—è —Ç—Ä—É–±–∫–∞ —Å–ª—É–∂–∏—Ç 5-15 –ª–µ—Ç (–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–ª—â–∏–Ω—ã —Å—Ç–µ–Ω–∫–∏), –∫–∞–ø–µ–ª—å–Ω–∞—è –ª–µ–Ω—Ç–∞ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –Ω–∞ 1-3 —Å–µ–∑–æ–Ω–∞; —Ç—Ä—É–±–∫–∞ —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ –º–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∏–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è–º, –ª–µ–Ω—Ç–∞ –±–æ–ª–µ–µ —É—è–∑–≤–∏–º–∞; —Ç—Ä—É–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–æ—Ä–º—É –±–µ–∑ –¥–∞–≤–ª–µ–Ω–∏—è, –ª–µ–Ω—Ç–∞ —Å–ø–ª—é—â–∏–≤–∞–µ—Ç—Å—è. –ö–∞–ø–µ–ª—å–Ω—ã–µ —Ç—Ä—É–±–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –æ—Ä–æ—à–µ–Ω–∏—è, –ª–µ–Ω—Ç—ã - –≤ —Å–µ–∑–æ–Ω–Ω—ã—Ö –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–∫–∞—Ö. –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –¥–∏–∞–º–µ—Ç—Ä –∫–∞–ø–µ–ª—å–Ω—ã—Ö —Ç—Ä—É–±–æ–∫ –æ–±—ã—á–Ω–æ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 16-20 –º–º, –¥–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã 0.7-4.0 –±–∞—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —ç–º–∏—Ç—Ç–µ—Ä–æ–≤.', 'source': None}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "import os\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# –í–∞—Ä–∏–∞–Ω—Ç 2: –µ—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ Google Drive\n",
        "# ------------------------------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # ‚Üê –£–±—Ä–∞–Ω –æ—Ç—Å—Ç—É–ø\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/data/dataset.json\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞\n",
        "print(\"–ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "We've just loaded the Google Sheet as a csv style Dataset, but we still need to format it into conversational style like below and then apply the chat template.\n",
        "\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
        "```\n",
        "\n",
        "We'll use a helper function `formatting_prompts_func` to do both!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "reoBXmAn7HlN"
      },
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]          # –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π\n",
        "    outputs = examples[\"output\"]\n",
        "\n",
        "    texts = []\n",
        "    for instr, inp, out in zip(instructions, inputs, outputs):\n",
        "        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Å—Ç–∏–ª–µ Granite / LLaMA\n",
        "        if inp.strip():  # –µ—Å–ª–∏ input –Ω–µ –ø—É—Å—Ç–æ–π\n",
        "            prompt = f\"### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:\\n{instr}\\n\\n### –í–≤–æ–¥:\\n{inp}\\n\\n### –û—Ç–≤–µ—Ç:\\n{out}\"\n",
        "        else:\n",
        "            prompt = f\"### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:\\n{instr}\\n\\n### –û—Ç–≤–µ—Ç:\\n{out}\"\n",
        "        texts.append(prompt)\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i5Sx9In7vHi"
      },
      "source": [
        "We now look at the raw input data before formatting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q76PqXv9yKa0"
      },
      "source": [
        "And we see how the chat template transformed these conversations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95_Nn-89DhsL",
        "outputId": "9c9d7456-ca39-4c87-b64a-9bdd0982d770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: We found double BOS tokens - we shall remove one automatically.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",  # ‚Üê –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "    # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ ‚Äî —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—Ä—É—á–Ω—É—é:\n",
        "    formatting_func=lambda example: [\n",
        "        f\"### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:\\n{example['instruction']}\\n\\n### –í–≤–æ–¥:\\n{example['input']}\\n\\n### –û—Ç–≤–µ—Ç:\\n{example['output']}\"\n",
        "    ],\n",
        "    max_seq_length=1024,  # Increased max_seq_length to 1024\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –∫–æ—Ä–æ—Ç–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2, # Increased batch size to 2\n",
        "        gradient_accumulation_steps=4, # Decreased gradient accumulation steps to 16\n",
        "        warmup_steps=10,\n",
        "        max_steps=200,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=42,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # –æ—Ç–∫–ª—é—á–∞–µ—Ç W&B\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1NBUozV78l"
      },
      "source": [
        "Let's verify masking the instruction part is done! Let's print the 100th row again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "LtsMVtlkUhja",
        "outputId": "8a8c3462-ed44-4ec1-8a30-8ffea57ef571"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:\\nWhat materials are used to manufacture drip tape starter valves?\\n\\n### –û—Ç–≤–µ—Ç:\\nDrip tape starter valves are primarily manufactured from polypropylene (PP) material, which provides durability, resistance to aging, and anti-clogging properties. [[18]] The rubber seals used in some valve designs are typically made from specialized elastomers that maintain flexibility and sealing properties under irrigation conditions. [[16]] These materials are chosen for their resistance to UV radiation, chemical exposure from fertilizers, and ability to withstand typical irrigation pressures up to 1.5 bar. [[16]] The construction ensures long-term reliability and minimal maintenance requirements in agricultural and garden irrigation applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kyjy__m9KY3"
      },
      "source": [
        "Now let's print the masked out example - you should see only the answer is present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rD6fl8EUxnG",
        "outputId": "bd4831b4-ce14-4786-de89-3519409805d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not find '### –û—Ç–≤–µ—Ç:' in the tokenized input to perform masking.\n",
            "### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:\n",
            "What materials are used to manufacture drip tape starter valves?\n",
            "\n",
            "### –û—Ç–≤–µ—Ç:\n",
            "Drip tape starter valves are primarily manufactured from polypropylene (PP) material, which provides durability, resistance to aging, and anti-clogging properties. [[18]] The rubber seals used in some valve designs are typically made from specialized elastomers that maintain flexibility and sealing properties under irrigation conditions. [[16]] These materials are chosen for their resistance to UV radiation, chemical exposure from fertilizers, and ability to withstand typical irrigation pressures up to 1.5 bar. [[16]] The construction ensures long-term reliability and minimal maintenance requirements in agricultural and garden irrigation applications.\n"
          ]
        }
      ],
      "source": [
        "# Get input_ids from the dataset\n",
        "input_ids_example = trainer.train_dataset[100][\"input_ids\"]\n",
        "\n",
        "# Define the start string for the assistant's response\n",
        "response_start_string = \"### –û—Ç–≤–µ—Ç:\"\n",
        "\n",
        "# Tokenize the response start string to find its token representation\n",
        "# Use add_special_tokens=False to match how content within the full text was likely tokenized.\n",
        "response_separator_token_ids = tokenizer.encode(response_start_string, add_special_tokens=False)\n",
        "\n",
        "# Helper function to find a sublist within a list\n",
        "def find_sublist(haystack, needle):\n",
        "    n = len(needle)\n",
        "    for i in range(len(haystack) - n + 1):\n",
        "        if haystack[i:i + n] == needle:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "# Find the start index of the response separator token sequence in the full input_ids\n",
        "separator_start_token_idx = find_sublist(input_ids_example, response_separator_token_ids)\n",
        "\n",
        "labels_to_decode = []\n",
        "if separator_start_token_idx != -1:\n",
        "    # The actual answer starts immediately after the separator tokens.\n",
        "    # Mask everything up to and including the separator.\n",
        "    answer_start_token_idx = separator_start_token_idx + len(response_separator_token_ids)\n",
        "    labels_to_decode = [-100] * answer_start_token_idx + list(input_ids_example[answer_start_token_idx:])\n",
        "else:\n",
        "    # Fallback if the separator is not found, indicating an unexpected format.\n",
        "    # In this case, we can't perform the masking as intended.\n",
        "    # For display, we'll just show the full input_ids, or a message.\n",
        "    # The instruction is to show the masked part, so if not found, we state it.\n",
        "    print(f\"Could not find '{response_start_string}' in the tokenized input to perform masking.\")\n",
        "    labels_to_decode = input_ids_example # Fallback to show something\n",
        "\n",
        "# Decode the generated labels, replacing -100 with tokenizer.pad_token for visibility.\n",
        "# Then replace actual pad_token string with a space for cleaner output.\n",
        "decoded_masked_labels_str = tokenizer.decode(\n",
        "    [tokenizer.pad_token_id if x == -100 else x for x in labels_to_decode]\n",
        ").replace(tokenizer.pad_token, \" \")\n",
        "\n",
        "print(decoded_masked_labels_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "4dcf9acd-b033-4f36-a93d-e8ddd08ac511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "1.32 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNP1Uidk9mrz"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`\n",
        "\n",
        "```\n",
        "Notice you might have to wait ~10 minutes for the Mamba kernels to compile! Please be patient!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "bfa08b86-bb37-4ecc-c69d-3c88980da6ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 203 | Num Epochs = 29 | Total steps = 200\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 16\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 16 x 1) = 32\n",
            " \"-____-\"     Trainable parameters = 6,651,904 of 359,031,808 (1.85% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  5/200 00:41 < 44:52, 0.07 it/s, Epoch 0.63/29]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.826100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.679700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.671300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-773422404.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Return inference mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2329\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_activation_offload_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2846\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2848\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2849\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model via Unsloth native inference! We'll use some example snippets not contained in our training data to get a sense of what was learned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk4A3U4l-SjD"
      },
      "outputs": [],
      "source": [
        "# @title Test Scenarios\n",
        "# --- Scenario 1: Video-Conferencing Screen-Share Bug (11 turns) ---\n",
        "scenario_1 = \"\"\"\n",
        "User: Everyone in my meeting just sees a black screen when I share.\n",
        "Agent: Sorry about that‚Äîare you sharing a window or your entire screen?\n",
        "User: Entire screen on macOS Sonoma.\n",
        "Agent: Thanks. Do you have ‚ÄúEnable hardware acceleration‚Äù toggled on in Settings ‚Üí Video?\n",
        "User: Yeah, that switch is on.\n",
        "Agent: Could you try toggling it off and start a quick test share?\n",
        "User: Did that‚Äîstill black for attendees.\n",
        "Agent: Understood. Are you on the desktop app v5.4.2 or the browser client?\n",
        "User: Desktop v5.4.2‚Äîjust updated this morning.\n",
        "\"\"\"\n",
        "\n",
        "# --- Scenario 2: Smart-Lock Low-Battery Loop (9 turns) ---\n",
        "scenario_2 = \"\"\"\n",
        "User: I changed the batteries, but the lock app still says 5 % and won‚Äôt auto-lock.\n",
        "Agent: Let‚Äôs check firmware. In the app, go to Settings ‚Üí Device Info‚Äîwhat version shows?\n",
        "User: 3.18.0-alpha.\n",
        "Agent: Latest stable is 3.17.5. Did you enroll in the beta program?\n",
        "User: I might have months ago.\n",
        "Agent: Beta builds sometimes misreport battery. Remove one battery, wait ten seconds, reinsert, and watch the LED pattern.\n",
        "User: LED blinks blue twice, then red once.\n",
        "Agent: That blink code means ‚Äúconfig mismatch.‚Äù Do you still have the old batteries handy?\n",
        "User: Tossed them already.\n",
        "\"\"\"\n",
        "\n",
        "# --- Scenario 3: Accounting SaaS ‚Äî Corrupted Invoice Export (10 turns) ---\n",
        "scenario_3 = \"\"\"\n",
        "User: Every invoice I download today opens as a blank PDF.\n",
        "Agent: Is this happening to historic invoices, new ones, or both?\n",
        "User: Both. Anything I export is 0 bytes.\n",
        "Agent: Are you exporting through ‚ÄúBulk Actions‚Äù or individual invoice pages?\n",
        "User: Individual pages.\n",
        "Agent: Which browser/OS combo?\n",
        "User: Chrome on Windows 11, latest update.\n",
        "Agent: We released a new PDF renderer at 10 a.m. UTC. Could you try Edge quickly, just to rule out a caching quirk?\n",
        "User: Tried Edge‚Äîsame zero-byte file.\n",
        "\"\"\"\n",
        "\n",
        "# --- Scenario 4: Fitness-Tracker App ‚Äî Stuck Step Count (8 turns) ---\n",
        "scenario_4 = \"\"\"\n",
        "User: My step count has been frozen at 4,237 since last night.\n",
        "Agent: Which phone are you syncing with?\n",
        "User: iPhone 15, iOS 17.5.\n",
        "Agent: In the Health Permissions screen, does ‚ÄúMotion & Fitness‚Äù show as ON?\n",
        "User: Yes, it‚Äôs toggled on.\n",
        "Agent: When you pull down to refresh the dashboard, does the sync spinner appear?\n",
        "User: Spinner flashes for a second, then nothing changes.\n",
        "\"\"\"\n",
        "\n",
        "# --- Scenario 5: Online-Course Platform ‚Äî Quiz Submission Error (12 turns) ---\n",
        "scenario_5 = \"\"\"\n",
        "User: My quiz submits but then shows ‚ÄúUnknown grading error‚Äù and resets the answers.\n",
        "Agent: Which course and quiz name?\n",
        "User: History 301, Unit 2 Quiz.\n",
        "Agent: Do you notice a red banner or any code like GR-### in the corner?\n",
        "User: Banner says ‚ÄúGR-412‚Äù.\n",
        "Agent: That code points to answer-payload size. Were you pasting images or long text into any answers?\n",
        "User: Maybe a long essay‚Äîabout 800 words in Question 5.\n",
        "Agent: Are you on a laptop or mobile?\n",
        "User: Laptop, Safari on macOS.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)  # –í–∫–ª—é—á–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ\n",
        "\n",
        "# –í—Ä—É—á–Ω—É—é —Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç (–±–µ–∑ input)\n",
        "prompt = f\"### Instruction:\\n{scenario_1}\\n\\n### Response:\\n\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=1024, # Aligned with training max_seq_length\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=1024,\n",
        "    use_cache=True,\n",
        "    do_sample=True,\n",
        "    temperature=0,\n",
        "    top_p=0.8,\n",
        "    top_k=20,\n",
        "    pad_token_id=tokenizer.pad_token_id,  # –≤–∞–∂–Ω–æ!\n",
        "    eos_token_id=tokenizer.eos_token_id,  # –≤–∞–∂–Ω–æ!\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGSmEnAd-sOP"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": scenario_2},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    padding = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = False)\n",
        "\n",
        "_ = model.generate(**inputs,\n",
        "                   streamer = text_streamer,\n",
        "                   max_new_tokens = 512, # Increase if tokens are getting cut off\n",
        "                   use_cache = True,\n",
        "                   # Adjust the sampling params to your preference\n",
        "                   do_sample=False,\n",
        "                   temperature = 0.7, top_p = 0.8, top_k = 20,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q974YEVPI7JS"
      },
      "source": [
        "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgcJIhJ0I_es"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzmjlc3gzJVs"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}